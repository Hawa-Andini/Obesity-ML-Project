# -*- coding: utf-8 -*-
"""ANN..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15OVLqGqgvycprh5djtX03TTDDn7sSmAp
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# Generate random data
X = np.random.rand(2111, 17).astype(np.float32)
y = np.random.randint(0, 2, size=(2111,)).astype(np.float32)  # Binary labels

# Convert to PyTorch tensors
X_tensor = torch.tensor(X)
y_tensor = torch.tensor(y).view(-1, 1)  # Reshape for compatibility

# Create DataLoader
dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)

class ANN(nn.Module):
    def __init__(self, input_size, activation_type="sigmoid"):
        super(ANN, self).__init__()
        self.fc1 = nn.Linear(input_size, 8)
        self.fc2 = nn.Linear(8, 4)
        self.fc3 = nn.Linear(4, 1)

        # Pilihan aktivasi untuk output layer
        self.activation_type = activation_type

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        if self.activation_type == "sigmoid":
            x = torch.sigmoid(self.fc3(x))  # Sigmoid
        elif self.activation_type == "tanh":
            x = torch.tanh(self.fc3(x))  # Tanh
        elif self.activation_type == "linear":
            x = self.fc3(x)  # Linear (tanpa aktivasi)
        elif self.activation_type == "sign":
            x = torch.sign(self.fc3(x))  # Sign Function (-1 atau 1)
        elif self.activation_type == "softplus":
            x = F.softplus(self.fc3(x))  # SoftPlus Activation

        return x

def train_model(activation_type):
    print(f"\nTraining dengan output activation: {activation_type}")
    model = ANN(input_size=17, activation_type=activation_type)

    # Loss function tergantung jenis aktivasi
    if activation_type == "sigmoid":
        criterion = nn.BCELoss()  # Untuk klasifikasi biner
    elif activation_type == "sign":
        criterion = nn.MSELoss()  # Untuk membandingkan -1 dan 1 dengan target
    else:
        criterion = nn.MSELoss()  # Untuk tanh dan linear

    optimizer = optim.Adam(model.parameters(), lr=0.01)

    num_epochs = 10
    for epoch in range(num_epochs):
        for batch_X, batch_y in dataloader:
            optimizer.zero_grad()
            batch_y = batch_y.float() * 2 - 1 if activation_type == "sign" else batch_y.float()
            output = model(batch_X)

            loss = criterion(output, batch_y)  # Compute loss
            loss.backward()
            optimizer.step()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

    return model

# Train dengan berbagai output layer
model_sigmoid = train_model("sigmoid")
model_tanh = train_model("tanh")
model_linear = train_model("linear")
model_sign = train_model("sign")
model_softplus = train_model("softplus")

# Contoh input untuk evaluasi
sample = torch.tensor([[0.2, 0.4, 0.6, 0.8, 0.1,
                        0.3, 0.5, 0.7, 0.9, 0.2,
                        0.4, 0.6, 0.8, 0.1, 0.3,
                        0.5, 0.7]], dtype=torch.float32)

# Evaluasi tanpa perhitungan gradien
with torch.no_grad():
    pred_sigmoid = model_sigmoid(sample).item()
    pred_tanh = model_tanh(sample).item()
    pred_linear = model_linear(sample).item()
    pred_sign = model_sign(sample).item()
    pred_softplus = model_softplus(sample).item()

print("\n=== Perbandingan Output ===")
print(f"Sigmoid Output: {pred_sigmoid:.4f}")  # Rentang (0,1)
print(f"Tanh Output   : {pred_tanh:.4f}")  # Rentang (-1,1)
print(f"Linear Output: {pred_linear:.4f}")  # Rentang bebas
print(f"Sign Output  : {pred_sign:.4f}")  # Hanya -1 atau 1
print(f"SoftPlus Output: {pred_softplus:.4f}")