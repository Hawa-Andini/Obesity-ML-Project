# -*- coding: utf-8 -*-
"""ENSEMBLE..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nG3X2bRa8hFuKNl7i595M256HPjSOyjc
"""

import numpy as np
from scipy.stats import binom

# Parameter
n_classifiers = 25  # Jumlah classifier dalam ensemble
epsilon = 0.35  # Error rate dari setiap classifier
majority_threshold = n_classifiers // 2 + 1  # Mayoritas suara (> 50%)

# 1. Simulasi jika classifier identik (error sama)
ensemble_error_identik = epsilon  # Sama seperti error classifier dasar

# 2. Simulasi jika classifier independen (error acak)
# Menghitung probabilitas lebih dari separuh classifier salah
ensemble_error_independen = sum(
    binom.pmf(k, n_classifiers, epsilon) for k in range(majority_threshold, n_classifiers + 1)
)

print(f"Error rate classifier individu: {epsilon:.2f}")
print(f"Error rate ensemble (classifier identik): {ensemble_error_identik:.2f}")
print(f"Error rate ensemble (classifier independen): {ensemble_error_independen:.2f}")

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

# Parameter
n_classifiers = 25  # Jumlah classifier dalam ensemble
base_errors = np.linspace(0, 1, 50)  # Error rate dari base classifier
ensemble_errors = []

# Hitung error ensemble untuk setiap error base classifier
for epsilon in base_errors:
    majority_threshold = n_classifiers // 2 + 1  # Mayoritas suara
    ensemble_error = sum(
        binom.pmf(k, n_classifiers, epsilon) for k in range(majority_threshold, n_classifiers + 1)
    )
    ensemble_errors.append(ensemble_error)

# Plot hasil
plt.figure(figsize=(6, 4))
plt.plot(base_errors, ensemble_errors, 'ko-', label='Ensemble Classifier Error')
plt.plot(base_errors, base_errors, 'k--', label='y = x (Reference Line)')
plt.xlabel("Base classifier error")
plt.ylabel("Ensemble classifier error")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score

# 1. Generate Dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                           n_redundant=5, random_state=42)

# 2. Split Data (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Inisialisasi Model
bagging_model = RandomForestClassifier(n_estimators=50, random_state=42)
adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)
gradientboost_model = GradientBoostingClassifier(n_estimators=50, random_state=42)

# 4. Training Model
bagging_model.fit(X_train, y_train)
adaboost_model.fit(X_train, y_train)
gradientboost_model.fit(X_train, y_train)

# 5. Prediksi
y_pred_bagging = bagging_model.predict(X_test)
y_pred_adaboost = adaboost_model.predict(X_test)
y_pred_gradientboost = gradientboost_model.predict(X_test)

# 6. Evaluasi Akurasi
accuracy_bagging = accuracy_score(y_test, y_pred_bagging)
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
accuracy_gradientboost = accuracy_score(y_test, y_pred_gradientboost)

# 7. Print Hasil
print(f"Random Forest (Bagging) Accuracy: {accuracy_bagging:.4f}")
print(f"AdaBoost (Boosting) Accuracy: {accuracy_adaboost:.4f}")
print(f"Gradient Boosting Accuracy: {accuracy_gradientboost:.4f}")

# 8. Visualisasi Perbandingan Akurasi
models = ['Random Forest (Bagging)', 'AdaBoost', 'Gradient Boosting']
accuracies = [accuracy_bagging, accuracy_adaboost, accuracy_gradientboost]

plt.figure(figsize=(8, 5))
plt.bar(models, accuracies, color=['blue', 'red', 'green'])
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.title('Comparison of Bagging vs Boosting Algorithms')
plt.show()

pip install ucimlrepo

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from ucimlrepo import fetch_ucirepo

# 1. Load Dataset dari UCI Repository
obesity = fetch_ucirepo(id=544)
df = obesity.data.original  # Menggunakan data asli dari UCI
print("Kolom dataset:", df.columns)  # Cek apakah 'NObeyesdad' ada

# 2. Encoding Data Kategorikal ke Numerik
label_encoders = {}
for col in df.columns:
    if df[col].dtype == 'object':  # Encode hanya jika tipe data object (kategori)
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

# 3. Pisahkan Fitur (X) dan Target (y)
X = df.drop(columns=['NObeyesdad'])
y = df['NObeyesdad']

# 4. Split Data (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Inisialisasi Model
bagging_model = RandomForestClassifier(n_estimators=50, random_state=42)
adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)
gradientboost_model = GradientBoostingClassifier(n_estimators=50, random_state=42)

# 6. Training Model
bagging_model.fit(X_train, y_train)
adaboost_model.fit(X_train, y_train)
gradientboost_model.fit(X_train, y_train)

# 7. Prediksi
y_pred_bagging = bagging_model.predict(X_test)
y_pred_adaboost = adaboost_model.predict(X_test)
y_pred_gradientboost = gradientboost_model.predict(X_test)

# 8. Evaluasi Akurasi
accuracy_bagging = accuracy_score(y_test, y_pred_bagging)
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)
accuracy_gradientboost = accuracy_score(y_test, y_pred_gradientboost)

# 9. Print Hasil
print(f"Random Forest (Bagging) Accuracy: {accuracy_bagging:.4f}")
print(f"AdaBoost (Boosting) Accuracy: {accuracy_adaboost:.4f}")
print(f"Gradient Boosting Accuracy: {accuracy_gradientboost:.4f}")

# 10. Visualisasi Perbandingan Akurasi
models = ['Random Forest (Bagging)', 'AdaBoost', 'Gradient Boosting']
accuracies = [accuracy_bagging, accuracy_adaboost, accuracy_gradientboost]

plt.figure(figsize=(8, 5))
plt.bar(models, accuracies, color=['blue', 'red', 'green'])
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.title('Comparison of Bagging vs Boosting on Obesity Dataset')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

# ðŸ”¹ Generate dataset dengan 2 fitur
X, y = make_classification(n_samples=300,
                           n_features=2,      # Total fitur = 2
                           n_informative=2,   # Semua fitur digunakan untuk informasi
                           n_redundant=0,     # Tidak ada fitur redundant
                           n_repeated=0,      # Tidak ada fitur repeated
                           n_classes=2,
                           n_clusters_per_class=1,
                           random_state=42)

# ðŸ”¹ Model Decision Tree tanpa Bagging
tree = DecisionTreeClassifier()
tree.fit(X, y)

# ðŸ”¹ Model dengan Bagging (menggunakan banyak Decision Trees)
bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)  # Perbaikan di sini!
bagging.fit(X, y)

# ðŸ”¹ Buat grid untuk visualisasi
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))

# ðŸ”¹ Prediksi menggunakan Decision Tree tunggal
Z_tree = tree.predict(np.c_[xx.ravel(), yy.ravel()])
Z_tree = Z_tree.reshape(xx.shape)

# ðŸ”¹ Prediksi menggunakan Bagging (Multiple Trees)
Z_bagging = bagging.predict(np.c_[xx.ravel(), yy.ravel()])
Z_bagging = Z_bagging.reshape(xx.shape)

# ðŸ”¹ Visualisasi hasil Decision Tree vs. Bagging
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Plot Decision Tree Tunggal
ax[0].contourf(xx, yy, Z_tree, alpha=0.3, cmap='coolwarm')
ax[0].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')
ax[0].set_title("Decision Tree Tunggal")

# Plot Bagging (Multiple Trees)
ax[1].contourf(xx, yy, Z_bagging, alpha=0.3, cmap='coolwarm')
ax[1].scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')
ax[1].set_title("Bagging (Multiple Decision Trees)")

plt.show()

# Install library jika belum terinstall
# !pip install ucimlrepo numpy pandas matplotlib scikit-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 1. Load Dataset dari UCI Repository
obesity = fetch_ucirepo(id=544)
df = obesity.data.original  # Menggunakan data asli dari UCI
print("Kolom dataset:", df.columns)  # Cek apakah 'NObeyesdad' ada

# 2. Encoding Data Kategorikal ke Numerik
label_encoders = {}
for col in df.columns:
    if df[col].dtype == 'object':  # Encode hanya jika tipe data object (kategori)
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

# 3. Pisahkan Fitur (X) dan Target (y)
X = df.drop(columns=['NObeyesdad'])
y = df['NObeyesdad']

# Split data: 80% training, 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ðŸ”¹ 3. Train Model Decision Tree Standalone
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)

# ðŸ”¹ 4. Train Model Bagging
bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)
bagging.fit(X_train, y_train)

# ðŸ”¹ 5. Train Model Boosting (AdaBoost)
boosting = AdaBoostClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)
boosting.fit(X_train, y_train)

# ðŸ”¹ 6. Prediksi
y_pred_tree = tree.predict(X_test)
y_pred_bagging = bagging.predict(X_test)
y_pred_boosting = boosting.predict(X_test)

# ðŸ”¹ 7. Evaluasi Akurasi
acc_tree = accuracy_score(y_test, y_pred_tree)
acc_bagging = accuracy_score(y_test, y_pred_bagging)
acc_boosting = accuracy_score(y_test, y_pred_boosting)

# ðŸ”¹ 8. Print hasil akurasi
print(f"Akurasi Decision Tree: {acc_tree:.4f}")
print(f"Akurasi Bagging: {acc_bagging:.4f}")
print(f"Akurasi Boosting: {acc_boosting:.4f}")

# ðŸ”¹ 9. Visualisasi hasil akurasi
models = ['Decision Tree', 'Bagging', 'Boosting']
accuracies = [acc_tree, acc_bagging, acc_boosting]

plt.figure(figsize=(8,5))
plt.bar(models, accuracies, color=['red', 'blue', 'green'])
plt.xlabel("Model")
plt.ylabel("Akurasi")
plt.ylim(0.8, 1)  # Fokus pada akurasi 0.8 - 1
plt.title("Perbandingan Akurasi: Decision Tree vs Bagging vs Boosting")
plt.show()

from ucimlrepo import fetch_ucirepo

# Fetch dataset Obesity Levels dari UCI Repository
obesity = fetch_ucirepo(id=544)

# Data (as pandas DataFrame)
X = obesity.data.features  # Fitur (input)
y = obesity.data.targets   # Target (label)

# Menampilkan metadata dataset
print(obesity.metadata)

# Menampilkan informasi variabel dalam dataset
print(obesity.variables)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score

# ðŸ”¹Load dataset dari URL UCI Machine Learning Repository
from ucimlrepo import fetch_ucirepo

# fetch dataset
bank_marketing = fetch_ucirepo(id=544)

# data (as pandas dataframes)
X = bank_marketing.data.features
y = bank_marketing.data.targets

# metadata
#print(bank_marketing.metadata)

# variable information
#print(bank_marketing.variables)

# ==========================
# ðŸ”¹ Preprocessing Data
# ==========================
# Konversi semua kolom kategorikal menjadi numerik
X = pd.get_dummies(X, drop_first=True)

# Bagi dataset menjadi training & testing (80:20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==========================
# ðŸ”¹ Model Training & Evaluation
# ==========================

# Model 1: Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
dt_acc = accuracy_score(y_test, dt_pred)

# Model 2: Random Forest (Bagging)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)

# Model 3: AdaBoost (Boosting)
ab_model = AdaBoostClassifier(n_estimators=100, random_state=42)
ab_model.fit(X_train, y_train)
ab_pred = ab_model.predict(X_test)
ab_acc = accuracy_score(y_test, ab_pred)

# ==========================
# ðŸ”¹ Hasil Perbandingan
# ==========================
print("\nðŸ“Š Perbandingan Akurasi Model:")
print(f"Decision Tree   : {dt_acc:.4f}")
print(f"Random Forest   : {rf_acc:.4f}")
print(f"AdaBoost        : {ab_acc:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from collections import Counter

# Dataset asli
X_original = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]).reshape(-1, 1)
y_original = np.array([1, 1, 1, -1, -1, -1, -1, 1, 1, 1])

# Parameter Bagging
n_estimators = 10  # Jumlah bootstrap samples
n_samples = len(X_original)  # Ukuran dataset
decision_stumps = []  # Menyimpan decision stumps
thresholds = []  # Menyimpan aturan keputusan

# Melakukan Bagging
for i in range(n_estimators):
    # Bootstrap Sampling (dengan pengambilan sampel ulang)
    indices = np.random.choice(range(n_samples), size=n_samples, replace=True)
    X_bootstrap, y_bootstrap = X_original[indices], y_original[indices]

    # Melatih Decision Stump (Pohon Keputusan dengan kedalaman 1)
    stump = DecisionTreeClassifier(max_depth=1)
    stump.fit(X_bootstrap, y_bootstrap)

    # Menyimpan model
    decision_stumps.append(stump)

    # Mencari threshold (nilai split)
    threshold = stump.tree_.threshold[0]  # Ambil nilai split pertama
    thresholds.append(threshold)

    # Menampilkan aturan keputusan
    print(f"Bagging Round {i+1}: x <= {threshold:.2f} => y = 1, x > {threshold:.2f} => y = -1")

# Prediksi menggunakan mayoritas voting
predictions = np.zeros((n_estimators, len(X_original)))

for i, stump in enumerate(decision_stumps):
    predictions[i] = stump.predict(X_original)

# Menghitung prediksi akhir dengan majority voting
final_predictions = []
for j in range(len(X_original)):
    majority_vote = Counter(predictions[:, j]).most_common(1)[0][0]  # Ambil label mayoritas
    final_predictions.append(majority_vote)

# Visualisasi hasil
plt.figure(figsize=(8, 5))
plt.scatter(X_original, y_original, label="True Labels", color="black", marker="o", s=80)
plt.scatter(X_original, final_predictions, label="Bagging Prediction", color="red", marker="x", s=80)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Bagging with Decision Stump")
plt.axhline(0, color='gray', linestyle="--")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
from ucimlrepo import fetch_ucirepo
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, StackingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

#  Load dataset dari UCI Machine Learning Repository
bank_marketing = fetch_ucirepo(id=544)

#  Data (as pandas dataframes)
X = bank_marketing.data.features
y = bank_marketing.data.targets

#  Preprocessing Data
# Jika target berbentuk dataframe, konversi menjadi series
if isinstance(y, pd.DataFrame):
    y = y.iloc[:, 0]

# Mengubah label target menjadi biner (yes = 1, no = 0) jika perlu
if y.dtype == 'O':  # Jika bertipe object (string)
    y = LabelEncoder().fit_transform(y)

# Mengubah semua variabel kategorikal menjadi numerik
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Normalisasi fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Model Classification
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Bagging (DT)": BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42),  # âœ… FIXED
    "Random Forest": RandomForestClassifier(n_estimators=50, random_state=42),
    "Extra Trees": ExtraTreesClassifier(n_estimators=50, random_state=42),
    "AdaBoost": AdaBoostClassifier(n_estimators=50, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=50, random_state=42),
    "Stacking": StackingClassifier(
        estimators=[
            ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
            ('svm', SVC(probability=True, random_state=42))
        ],
        final_estimator=LogisticRegression()
    ),
    "Voting": VotingClassifier(
        estimators=[
            ('dt', DecisionTreeClassifier(random_state=42)),
            ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
            ('gb', GradientBoostingClassifier(n_estimators=10, random_state=42))
        ],
        voting='soft'
    )
}

# Evaluasi Model
results = {}
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    results[name] = scores.mean()

# Menampilkan Hasil
df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy']).sort_values(by='Accuracy', ascending=False)
print(df_results)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from ucimlrepo import fetch_ucirepo

# Load dataset
#  Load dataset dari UCI Machine Learning Repository
bank_marketing = fetch_ucirepo(id=544)

#  Data (as pandas dataframes)
X = bank_marketing.data.features
y = bank_marketing.data.targets

#  Preprocessing Data
# Jika target berbentuk dataframe, konversi menjadi series
if isinstance(y, pd.DataFrame):
    y = y.iloc[:, 0]

# Mengubah label target menjadi biner (yes = 1, no = 0) jika perlu
if y.dtype == 'O':  # Jika bertipe object (string)
    y = LabelEncoder().fit_transform(y)

# Mengubah semua variabel kategorikal menjadi numerik
for col in X.select_dtypes(include=['object']).columns:
    X[col] = LabelEncoder().fit_transform(X[col])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize data for SVM and KNN
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define classifiers
adaboost_tree = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)
adaboost_svm = AdaBoostClassifier(estimator=SVC(probability=True, kernel='linear'), n_estimators=50, random_state=42)

gb_tree = GradientBoostingClassifier(n_estimators=50, random_state=42)
gb_knn = GradientBoostingClassifier(n_estimators=50, random_state=42)

# Train models
models = {
    "AdaBoost + Decision Tree": adaboost_tree,
    "AdaBoost + SVM": adaboost_svm,
    "Gradient Boosting + Decision Tree": gb_tree,
    "Gradient Boosting + KNN": gb_knn
}

accuracies = {}
for name, model in models.items():
    if "SVM" in name or "KNN" in name:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    accuracies[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")

# Plot accuracies
sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))
plt.xticks(rotation=45, ha='right')
plt.ylabel("Accuracy")
plt.title("Comparison of AdaBoost & Gradient Boosting with Different Base Classifiers")
plt.show()