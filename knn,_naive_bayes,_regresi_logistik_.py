# -*- coding: utf-8 -*-
"""KNN, Naive Bayes, Regresi Logistik..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R2hulzte7voIYHetHQTdztlyaNJQCTn_
"""

from google.colab import files
uploads = files.upload()

df = pd.read_csv('Dataset_Obesity.csv')
df.head()

df.info()

df.duplicated().sum()

# Install library yang diperlukan
!pip install ucimlrepo imbalanced-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_curve
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

# Load dataset
df = pd.read_csv('Dataset_Obesity.csv')

# Pisahkan fitur dan target
X = df.drop(columns=["NObeyesdad"])
y = df["NObeyesdad"]

# Identifikasi kolom numerik dan kategorik
num_cols = X.select_dtypes(include=["float64", "int64"]).columns.tolist()
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()

# Definisikan preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# Bagi dataset menjadi train dan test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Model yang akan diuji
model = DecisionTreeClassifier()

# Hyperparameter tuning untuk Decision Tree
param_grid = {
    "classifier__max_depth": [3, 5, 10, None],
    "classifier__criterion": ["gini", "entropy"]
}

# Cross-validation dengan Stratified K-Fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Pipeline dengan feature selection
pipeline = ImbPipeline([
    ('preprocessing', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),
    ('classifier', model)
])

# GridSearchCV untuk mencari hyperparameter terbaik
grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)

grid_search.fit(X_train, y_train)

# Evaluasi di training set untuk cek overfitting
train_acc = grid_search.best_estimator_.score(X_train, y_train)

# Evaluasi di test set
y_pred = grid_search.best_estimator_.predict(X_test)
y_proba = grid_search.best_estimator_.predict_proba(X_test) if hasattr(grid_search.best_estimator_, "predict_proba") else None
test_acc = accuracy_score(y_test, y_pred)

# Hitung AUC jika y_proba tersedia
if y_proba is not None:
    if y_proba.shape[1] > 1:  # Multi-class
        auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
        y_proba = y_proba[:, 1]  # Ambil probabilitas kelas positif untuk ROC Curve
    else:  # Binary classification
        auc = roc_auc_score(y_test, y_proba)
else:
    auc = None

# Print hasil evaluasi
print(f"‚úÖ Best Params: {grid_search.best_params_}")
print(f"‚úÖ Train Accuracy: {train_acc:.2f}")
print(f"‚úÖ Test Accuracy: {test_acc:.2f}")
print(f"‚úÖ AUC Score: {auc:.2f}")
print(f"‚úÖ Classification Report:\n{classification_report(y_test, y_pred)}")

# Plot Confusion Matrix
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Plot ROC Curve jika y_proba tersedia
if y_proba is not None:
    fpr, tpr, _ = roc_curve(y_test, y_proba, pos_label=y_test.unique()[1])
    plt.figure(figsize=(6, 5))
    plt.plot(fpr, tpr, label=f'Decision Tree (AUC = {auc:.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend()
    plt.show()

# Cetak hasil perbandingan model
print("\nüîç Perbandingan Model:")
comparison = pd.DataFrame({
    "Decision Tree": {
        "Train Accuracy": train_acc,
        "Test Accuracy": test_acc,
        "AUC": auc
    }
}).T
print(comparison)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix, roc_curve
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

# Load dataset
df = pd.read_csv('Dataset_Obesity.csv')

# Pisahkan fitur dan target
X = df.drop(columns=["NObeyesdad"])
y = df["NObeyesdad"]

# Identifikasi kolom numerik dan kategorik
num_cols = X.select_dtypes(include=["float64", "int64"]).columns.tolist()
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()

# Mendefinisikan preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# Bagi dataset menjadi train dan test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Fungsi untuk melatih dan mengevaluasi model
def train_evaluate_model(model, param_grid, model_name):
    pipeline = ImbPipeline([
        ('preprocessing', preprocessor),
        ('smote', SMOTE(random_state=42)),
        ('feature_selection', SelectKBest(score_func=f_classif, k=10)),
        ('classifier', model)
    ])

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    train_acc = grid_search.best_estimator_.score(X_train, y_train)
    y_pred = grid_search.best_estimator_.predict(X_test)
    y_proba = grid_search.best_estimator_.predict_proba(X_test) if hasattr(grid_search.best_estimator_, "predict_proba") else None
    test_acc = accuracy_score(y_test, y_pred)

    auc = roc_auc_score(y_test, y_proba, multi_class='ovr') if y_proba is not None else None

    print(f"\n‚úÖ {model_name} Best Params: {grid_search.best_params_}")
    print(f"‚úÖ Train Accuracy: {train_acc:.2f}")
    print(f"‚úÖ Test Accuracy: {test_acc:.2f}")
    print(f"‚úÖ AUC Score: {auc:.2f}")
    print(f"‚úÖ Classification Report:\n{classification_report(y_test, y_pred)}")

    plt.figure(figsize=(5, 4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    if y_proba is not None:
        fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1], pos_label=y_test.unique()[1])
        plt.figure(figsize=(6, 5))
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')
        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {model_name}")
        plt.legend()
        plt.show()

# KNN Model
knn_model = KNeighborsClassifier()
knn_param_grid = {"classifier__n_neighbors": [3, 5, 7], "classifier__weights": ["uniform", "distance"]}
train_evaluate_model(knn_model, knn_param_grid, "KNN")

# Naive Bayes Model
nb_model = GaussianNB()
nb_param_grid = {}
train_evaluate_model(nb_model, nb_param_grid, "Naive Bayes")

# Logistic Regression Model
lr_model = LogisticRegression(max_iter=1000)
lr_param_grid = {"classifier__C": [0.01, 0.1, 1, 10]}
train_evaluate_model(lr_model, lr_param_grid, "Logistic Regression")